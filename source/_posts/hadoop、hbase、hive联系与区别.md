title: hadoop、hbase、hive联系与区别
author: Wtli
tags:
  - 大数据
categories: []
date: 2021-05-19 14:25:00
---
在本文中，我们将讨论一个特定的数据管理工具系列，这些工具在讨论时经常会混淆，并且可以互换使用。今天我们将讨论Hadoop、HDFS、HBase和Hive，以及它们如何帮助我们处理和存储大量数据。

<!--more-->

![upload successful](/images/pasted-84.png)
  
#### Hadoop

Hadoop经常被用作一个通用术语，用于指代几种不同的技术。不过，Hadoop也是一个特定的软件框架。它使用户能够轻松地管理分布式计算和存储。它通过在多个存储中划分文档以及在机器集群中划分块来实现。

为了实现容错，Hadoop在集群上复制这些存储。然后，它通过将作业划分为几个较小的独立任务来执行分布式处理。然后在计算机集群上并行运行此任务。

Hadoop可以跨集群服务对大型数据集进行分布式处理，以同时在多台机器上工作。要在Hadoop上处理任何数据，需要使用几种服务，我们将讨论这些服务：

- HDFS: HDFS或Hadoop分布式文件系统（HDFS, or Hadoop Distributed File System），是一个主-从架构模式，它有两个运行的守护进程：DataNode和NameNode。
- MapReduce：这是一种在分布式集群上并行处理大数据的算法。然后MapReduce可以将这些数据组合成结果。
- YARN：YARN的功能是将 资源管理、 作业监视和调度任务划分为单独的守护进程。能管理上千个节点，这是因为 YARN 允许用户通过多个集群进入一个大集群。我们可以在一个更大的工作中使用多个独立的集群，通过更大规模的系统实现。 

#### HDFS

如前所述，HDFS是运行在两个守护进程DataNode和NameNode上的主从拓扑。

NameNode存储DataNodes中存储所有数据的元数据。此外，如果NameNode故障并没有任何备份，则整个Hadoop实例将无法访问。这有点像在遍历链表时丢失指针。如果您不知道接下来数据的存储位置，那么您将无法访问它。

DataNodes，是实际存储数据的节点。如果有某个DataNode故障，整个集群不会受影响，因为NameNode经常管理跨数据节点的同一数据块的多个实例（根据配置）。

使用Hadoop分布式文件系统，您可以在服务器上写入一次数据，然后多次读取。HDFS是立即处理大量数据的理想选择。原因是HDFS与硬件群集中的NameNode和协同工作DataNodes。

您可以在便宜的硬件上运行HDFS并轻松地水平扩展（这意味着购买更多的机器来处理数据），这一事实使它成为非常受欢迎的选择。以前，大多数公司都依靠垂直扩展（购买通常很昂贵但可以单独处理更多数据的服务器）。这是昂贵的，并且具有更多的计算限制。

HDFS和Hadoop与MapReduce等其他基础层组件相结合，使各种规模和能力的企业都可以扩展其数据处理能力，而无需购买昂贵的设备。

#### HBase

HDFS和Hadoop在开发的时候是很相似的。但是，HBase是非常不同的。

HBase是建立在Hadoop文件系统之上的开源，面向列的数据库。在某种程度上，这是人们更熟悉的层，从某种意义上说，它与典型的数据库非常相似。

它是水平可扩展的。HBase的数据模型类似于Google大表设计的数据模型。它不仅提供对大量非结构化数据的快速随机访问，而且还利用了HDFS提供的相同的容错能力。

HBase是Hadoop生态系统的一部分，该生态系统实时提供对Hadoop文件系统中数据的读写访问。由于相同的原因，许多大公司都将HBase用于其日常功能。例如，Pinterest可与38个HBase群集一起工作，每秒执行约500万次操作！

更重要的是，HBase提供了对一百万条记录中的单个行的较低延迟访问。为了正常工作，HBase在内部使用哈希表，然后提供对索引的HDFS文件的随机访问。

#### Hive


尽管Hadoop具有非常好的可扩展性和可靠性，并且非常适合提取数据，但是它的学习曲线过于陡峭，无法使其具有成本效益和时间效益。另一个很好的替代方法是在MapReduce之上的Apache Hive。

Hive是一种数据仓库软件，允许用户快速轻松地编写类似SQL的查询以从Hadoop提取数据。

这个开源框架的主要目的是处理和存储大量数据。对于Hadoop，您可以使用MapReduce Java API实施SQL查询。对于Apache Hive，您可以轻松绕过Java，而仅使用类似SQL的查询访问数据。

Apache Hive的工作很简单。它将用HiveQL编写的输入程序转换为一个或多个Java的MapReduce和Spark作业。

然后，它将数据组织到HDFS表中，并在群集上运行作业以产生结果。Hive是将结构应用于大量非结构化数据，然后对它们执行基于SQL的查询的一种简单方法。由于它使用的接口熟悉JDBC（Java数据库连接），因此可以轻松地与传统数据中心技术集成。

Hive的一些最重要的组件是：

- MetaStore：这是存储Hive表的架构。Hive Metastore主要用于保存有关仓库中分区和表的所有信息。默认情况下，它运行与Hive服务相同的过程。
- SerDe： SerDe或序列化器/反序列化器是一项功能，向配置单元提供有关如何处理记录的指令

#### 综述

这些事情不尽相同-但它们可以协同工作
我们已经讨论了Hadoop，Hive，HBase和HDFS。所有这些开源工具和软件都旨在帮助处理和存储大数据并从中获得有用的见解。
即使它们都扮演非常不同的角色，它们也经常互换使用。

总而言之，Hadoop用作文件存储框架，该框架又将HDFS用作主-从拓扑结构，以将文件存储在Hadoop环境中。

然后，HBase作为基于列的分布式数据库系统（如Google的Big Table）构建在HDFS之上，这对于随机访问Hadoop文件非常有用。

另一方面，Hive提供了一个基于Hadoop的类似SQL的接口，以绕过JAVA编码。

附：[参考网站](https://betterprogramming.pub/hadoop-vs-hdfs-vs-hbase-vs-hive-ddfffd45d222)